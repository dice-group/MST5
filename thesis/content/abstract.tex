\begin{abstract}
        % Purpose
        The desire for knowledge is one of the most important characteristics of humans. 
        Since knowledge graphs store knowledge in a graph structure rather than plain text, 
        logical query can be applied to obtain implicit information, which is a more efficient and effective way. 
        Many researchers published approaches for converting English natural language questions to SPARQL queries, 
        which are used to query Wikidata and DBpedia. 
        However, multilingual question answering systems so far perform well on English, 
        but not on other languages, especially on languages with missing training data. 
        % Methods
        In this paper, we develop a knowledge-graph multilingual question-answering system using the mt5 model from google, 
        a pre-trained transformer model, 
        which supports 101 languages. 
        Our system treats SPARQL queries as another language and converts natural language questions to SPARQL queries. 
        We explore zero-shot along with different fine-tuning strategies to reach the best performance. 
        % Results
        Our experiment results show the state-of-the-art performance of the system not only on English, 
        but also on other languages for QALD-9-plus dataset. 
        % Implications
        
        \keywords{Transformers \and KGQA \and Wikidata}
\end{abstract}