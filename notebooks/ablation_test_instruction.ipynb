{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create LcQUAD2.0 training datasets for ablation**\n",
    "\n",
    "Only natural language queries:\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/lcquad2/train.json \\\n",
    "-o datasets/lcquad2/train-simple.csv \\\n",
    "-t lcquad2 \\\n",
    "--question_padding_length 128\n",
    "```\n",
    "\n",
    "Natural language queries + linguistic context:\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/lcquad2/train.json \\\n",
    "-o datasets/lcquad2/train-lc.csv \\\n",
    "-t lcquad2 \\\n",
    "--linguistic_context \\\n",
    "--question_padding_length 128\n",
    "```\n",
    "\n",
    "Natural language queries + entity information:\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/lcquad2/train.json \\\n",
    "-o datasets/lcquad2/train-ent.csv \\\n",
    "-t lcquad2 \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 128 \\\n",
    "--entity_padding_length 64\n",
    "```\n",
    "\n",
    "Natural language queries + linguistic context + entity information:\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/lcquad2/train.json \\\n",
    "-o datasets/lcquad2/train-lc-ent.csv \\\n",
    "-t lcquad2 \\\n",
    "--linguistic_context \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 128 \\\n",
    "--entity_padding_length 64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tune mT5-xl on LCQUAD2**\n",
    "\n",
    "Only natural language queries:\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60005 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero2.json \\\n",
    "    --model_name_or_path google/mt5-xl \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/lcquad2/train-simple.csv \\\n",
    "    --output_dir fine-tuned_models/lcquad2-finetune-simple\\\n",
    "    --num_train_epochs 15 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name lcquad2-finetune-simple \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```\n",
    "\n",
    "Natural language queries + linguistic context:\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60010 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero2.json \\\n",
    "    --model_name_or_path google/mt5-xl \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/lcquad2/train-lc.csv \\\n",
    "    --output_dir fine-tuned_models/lcquad2-finetune-lc \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name lcquad2-finetune-lc \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```\n",
    "\n",
    "Natural language queries + entity information:\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60015 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero2.json \\\n",
    "    --model_name_or_path google/mt5-xl \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/lcquad2/train-ent.csv \\\n",
    "    --output_dir fine-tuned_models/lcquad2-finetune-ent \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name lcquad2-finetune-ent \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```\n",
    "\n",
    "Natural language queries + linguistic context + entity information:\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero2.json \\\n",
    "    --model_name_or_path google/mt5-xl \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/lcquad2/train-lc-ent.csv \\\n",
    "    --output_dir fine-tuned_models/lcquad2-finetune-lc-ent \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name lcquad2-finetune-lc-ent \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create QALD9Plus training datasets for ablation**\n",
    "\n",
    "Only natural language queries:\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.json \\\n",
    "-o datasets/qald9plus/wikidata/qald_9_plus_train_wikidata-simple.csv \\\n",
    "-t qald \\\n",
    "-kg Wikidata \\\n",
    "-l all \\\n",
    "--question_padding_length 128\n",
    "```\n",
    "\n",
    "Natural language queries + linguistic context:\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.json \\\n",
    "-o datasets/qald9plus/wikidata/qald_9_plus_train_wikidata-lc.csv \\\n",
    "-t qald \\\n",
    "-kg Wikidata \\\n",
    "-l all \\\n",
    "--linguistic_context \\\n",
    "--question_padding_length 128\n",
    "```\n",
    "\n",
    "Natural language queries + entity information:\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.json \\\n",
    "-o datasets/qald9plus/wikidata/qald_9_plus_train_wikidata-ent.csv \\\n",
    "-t qald \\\n",
    "-kg Wikidata \\\n",
    "-l all \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 128 \\\n",
    "--entity_padding_length 64\n",
    "```\n",
    "\n",
    "Natural language queries + linguistic context + entity information:\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.json \\\n",
    "-o datasets/qald9plus/wikidata/qald_9_plus_train_wikidata-lc-ent.csv \\\n",
    "-t qald \\\n",
    "-kg Wikidata \\\n",
    "-l all \\\n",
    "--linguistic_context \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 128 \\\n",
    "--entity_padding_length 64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Fine-tune on QALD9Plus**\n",
    "\n",
    "Only natural language queries:\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60005 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero2.json \\\n",
    "    --model_name_or_path fine-tuned_models/lcquad2-finetune-simple \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/qald9plus/wikidata/qald_9_plus_train_wikidata-simple.csv \\\n",
    "    --output_dir fine-tuned_models/qald9plus-finetune-simple \\\n",
    "    --num_train_epochs 32 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name qald9plus-finetune-simple \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```\n",
    "\n",
    "\n",
    "Natural language queries + linguistic context:\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60010 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero2.json \\\n",
    "    --model_name_or_path fine-tuned_models/lcquad2-finetune-lc \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/qald9plus/wikidata/qald_9_plus_train_wikidata-lc.csv \\\n",
    "    --output_dir fine-tuned_models/qald9plus-finetune-lc \\\n",
    "    --num_train_epochs 32 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name qald9plus-finetune-lc \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```\n",
    "\n",
    "\n",
    "Natural language queries + entity information:\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60015 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero2.json \\\n",
    "    --model_name_or_path fine-tuned_models/lcquad2-finetune-ent \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/qald9plus/wikidata/qald_9_plus_train_wikidata-ent.csv \\\n",
    "    --output_dir fine-tuned_models/qald9plus-finetune-ent \\\n",
    "    --num_train_epochs 32 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name qald9plus-finetune-ent \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```\n",
    "\n",
    "\n",
    "Natural language queries + linguistic context + entity information:\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero2.json \\\n",
    "    --model_name_or_path fine-tuned_models/lcquad2-finetune-lc-ent \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/qald9plus/wikidata/qald_9_plus_train_wikidata-lc-ent.csv \\\n",
    "    --output_dir fine-tuned_models/qald9plus-finetune-lc-ent \\\n",
    "    --num_train_epochs 32 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name qald9plus-finetune-lc-ent \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
