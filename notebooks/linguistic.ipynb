{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/m/mengshim/profiles/unix/cs/miniconda3/envs/mengshi/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/upb/users/m/mengshim/profiles/unix/cs/miniconda3/envs/mengshi/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What What PRON attr\n",
      "'s 's AUX ROOT\n",
      "the the DET det\n",
      "name name NOUN nsubj\n",
      "of of ADP prep\n",
      "the the DET det\n",
      "president president NOUN pobj\n",
      "of of ADP prep\n",
      "united united PROPN compound\n",
      "states states PROPN pobj\n",
      "of of ADP prep\n",
      "America America PROPN pobj\n",
      "and and CCONJ cc\n",
      "embedding embedding VERB conj\n",
      "? ? PUNCT punct\n",
      "['▁What', \"'\", 's', '▁the', '▁name', '▁of', '▁the', '▁president', '▁of', '▁', 'united', '▁', 'states', '▁of', '▁America', '▁and', '▁', 'embed', 'ding', '▁', '?']\n",
      "[5126, 277, 263, 287, 6535, 304, 287, 13243, 304, 259, 82011, 259, 52495, 304, 10055, 305, 259, 30222, 10646, 259, 291]\n"
     ]
    }
   ],
   "source": [
    "from linguistic_parser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What's the name of the president of united states of America and embedding ?\"\n",
    "text_zh = \"美国总统的名字是什么？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_zh = spacy.load(\"zh_core_web_sm\")\n",
    "nlp_ja = spacy.load(\"ja_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(What, 'PRON', 'attr', 2),\n",
       " ('s, 'AUX', 'ROOT', 1),\n",
       " (the, 'DET', 'det', 3),\n",
       " (name, 'NOUN', 'nsubj', 2),\n",
       " (of, 'ADP', 'prep', 3),\n",
       " (the, 'DET', 'det', 5),\n",
       " (president, 'NOUN', 'pobj', 4),\n",
       " (of, 'ADP', 'prep', 5),\n",
       " (united, 'PROPN', 'compound', 7),\n",
       " (states, 'PROPN', 'pobj', 6),\n",
       " (of, 'ADP', 'prep', 7),\n",
       " (America, 'PROPN', 'pobj', 8),\n",
       " (and, 'CCONJ', 'cc', 2),\n",
       " (embedding, 'VERB', 'conj', 2),\n",
       " (?, 'PUNCT', 'punct', 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What's the name of the president of united states of America and embedding ?\"\n",
    "doc = get_doc(text, nlp_en)\n",
    "pos = get_pos(doc)\n",
    "dep = get_dep(doc)\n",
    "root = get_root_node(doc, dep)\n",
    "depth_list = get_dep_depth(root, [-1] * len(doc))\n",
    "\n",
    "linguistic = zip(doc, pos, dep, depth_list)\n",
    "list(linguistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(桑尼, 'NOUN', 'conj', 3),\n",
       " (和, 'CCONJ', 'cc', 3),\n",
       " (雪儿, 'NOUN', 'nmod:assmod', 3),\n",
       " (的, 'PART', 'case', 4),\n",
       " (儿子, 'NOUN', 'nsubj', 2),\n",
       " (是, 'VERB', 'cop', 2),\n",
       " (谁, 'PRON', 'ROOT', 1),\n",
       " (？, 'PUNCT', 'punct', 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"桑尼和雪儿的儿子是谁？\"\n",
    "doc = get_doc(text, nlp_zh)\n",
    "pos = get_pos(doc)\n",
    "dep = get_dep(doc)\n",
    "root = get_root_node(doc, dep)\n",
    "depth_list = get_dep_depth(root, [-1] * len(doc))\n",
    "\n",
    "linguistic = zip(doc, pos, dep, depth_list)\n",
    "list(linguistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '美国', '总统', '的名字', '是什么', '?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NNQT_question': 'What is the {periodical literature} for {mouthpiece} of {Delta Air Lines}', 'uid': 19719, 'subgraph': 'simple question right', 'template_index': 65, 'question': 'What periodical literature does Delta Air Lines use as a moutpiece?', 'sparql_wikidata': ' select distinct ?obj where { wd:Q188920 wdt:P2813 ?obj . ?obj wdt:P31 wd:Q1002697 } ', 'sparql_dbpedia18': 'select distinct ?obj where { ?statement <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> <http://wikidata.dbpedia.org/resource/Q188920> . ?statement <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> <http://www.wikidata.org/entity/P2813> . ?statement <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?obj . ?obj <http://www.wikidata.org/entity/P31> <http://wikidata.dbpedia.org/resource/Q1002697> } ', 'template': ' <S P ?O ; ?O instanceOf Type>', 'answer': [], 'template_id': 1, 'paraphrased_question': \"What is Delta Air Line's periodical literature mouthpiece?\", 'pos': ['PRON', 'AUX', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'PROPN'], 'dep': ['attr', 'ROOT', 'det', 'amod', 'nsubj', 'prep', 'pobj', 'prep', 'compound', 'compound', 'pobj'], 'dep_depth': [2, 1, 3, 3, 2, 3, 4, 5, 7, 7, 6]}\n"
     ]
    }
   ],
   "source": [
    "from preprocess import read_json\n",
    "dataset = read_json(\"../datasets/lcquad/train_linguistic.json\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"桑尼和雪儿的儿子是谁？\"\n",
    "doc = get_doc(text, nlp_zh)\n",
    "pos = get_pos(doc)\n",
    "dep = get_dep(doc)\n",
    "root = get_root_node(doc, dep)\n",
    "depth_list = get_dep_depth(root, [-1] * len(doc))\n",
    "\n",
    "linguistic = zip(doc, pos, dep, depth_list)\n",
    "list(linguistic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mengshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
