{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions to train the model\n",
    "\n",
    "#### **Model attributes**\n",
    "- uses Wikidata knowledge graph\n",
    "- uses (pre-trained) mT5-XL as base model\n",
    "- pre-trained on LC-QuAD 2.0 for 15 epochs\n",
    "- fine-tuned on QALD-9-Plus (custom) on all languages for 15 epochs\n",
    "- utilizes linguistic context, entity knowledge and padding in pre-training and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pre-training:\n",
    "\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/lcquad2/train.json \\\n",
    "-o datasets/lcquad2/train.csv \\\n",
    "-t lcquad2 \\\n",
    "--linguistic_context \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 32 \\\n",
    "--entity_padding_length 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning:\n",
    "\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.json \\\n",
    "-o datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.csv \\\n",
    "-t qald \\\n",
    "-kg Wikidata \\\n",
    "-l all \\\n",
    "--linguistic_context \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 32 \\\n",
    "--entity_padding_length 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pre-train on LC-QuAD 2.0**\n",
    "\n",
    "Since pre-training and fine-tuning commands are based on the same script `train_ds.sh`, in order to save time and avoid errors, we directly provide the code in the configured bash script. You can run the following command in your terminal.\n",
    "\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero3.json \\\n",
    "    --model_name_or_path google/mt5-xl \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/lcquad2/train.csv \\\n",
    "    --output_dir fine-tuned_models/lcquad2-pretrain \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 6000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name lcquad2-pretrain \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fine-tune on QALD-9-Plus**\n",
    "\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero3.json \\\n",
    "    --model_name_or_path fine-tuned_models/lcquad2-pretrain \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.csv \\\n",
    "    --output_dir fine-tuned_models/qald9plus-finetune-new \\\n",
    "    --num_train_epochs 32 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 3000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name qald9plus-finetune-new \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Continue training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero3.json \\\n",
    "    --model_name_or_path fine-tuned_models/qald9plus-finetune-new \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.csv \\\n",
    "    --output_dir fine-tuned_models/qald9plus-finetune-new-2 \\\n",
    "    --num_train_epochs 70 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 500 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name qald9plus-finetune-new-2 \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GERBIL Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `eval.sh` to generate prediction files in QALD format and evaluate them with GERBIL.\n",
    "`eval.sh` is configured. \n",
    "\n",
    "```bash\n",
    "./eval.sh\n",
    "```\n",
    "\n",
    "Prediction files are stored in `pred_files/qald9plus-finetune`.\n",
    "The script uploads them to GERBIL along with the reference test file\n",
    "and waits for 5 minutes for the results.\n",
    "If the GERBIL experiment terminates, the results are stored in `pred_files/qald9plus-finetune/result.csv`, else, the experiment id is stored in this file. You can use the following commands to generate a csv files for results:\n",
    "\n",
    "```bash\n",
    "python3 code/gerbil_eval.py --experiment_id [experiment_id] --pred_path pred_files/qald9plus-finetune\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
