{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions to train the model\n",
    "\n",
    "#### **Model attributes**\n",
    "- uses DBpedia knowledge graph\n",
    "- uses (pre-trained) mT5-XL as base model\n",
    "- pre-trained on LC-QuAD 1.0 for 32 epochs\n",
    "- fine-tuned on QALD-9-Plus (custom) on all languages for 15 epochs\n",
    "- utilizes linguistic context, entity knowledge and padding in pre-training and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pre-training:\n",
    "\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/lcquad1/train-data.json \\\n",
    "-o datasets/lcquad1/train-data.csv \\\n",
    "-t lcquad1 \\\n",
    "--linguistic_context \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 32 \\\n",
    "--entity_padding_length 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning:\n",
    "\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/qald9plus/dbpedia/qald_9_plus_train_dbpedia.json \\\n",
    "-o datasets/qald9plus/dbpedia/qald_9_plus_train_dbpedia.csv \\\n",
    "-t qald \\\n",
    "-kg DBpedia \\\n",
    "-l all \\\n",
    "--linguistic_context \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 32 \\\n",
    "--entity_padding_length 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pre-train on LC-QuAD 1.0**\n",
    "\n",
    "Since pre-training and fine-tuning commands are based on the same script `train_ds.sh`, in order to save time and avoid errors, we directly provide the code in the configured bash script. You can run the following command in your terminal.\n",
    "\n",
    "```bash\n",
    "deepspeed --include=localhost:1 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero3.json \\\n",
    "    --model_name_or_path google/mt5-xl \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/lcquad1/train-data.csv \\\n",
    "    --output_dir fine-tuned_models/dbpedia/lcquad1-pretrain \\\n",
    "    --num_train_epochs 32 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 6000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name lcquad1-pretrain \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fine-tune on QALD-9-Plus**\n",
    "\n",
    "```bash\n",
    "deepspeed --include=localhost:1 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero3.json \\\n",
    "    --model_name_or_path fine-tuned_models/dbpedia/lcquad1-pretrain \\\n",
    "    --do_train \\\n",
    "    --train_file  datasets/qald9plus/dbpedia/qald_9_plus_train_dbpedia.csv \\\n",
    "    --output_dir fine-tuned_models/dbpedia/qald9plus-finetune \\\n",
    "    --num_train_epochs 32 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 3000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name dbpedia-qald9plus-finetune \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Continue Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "deepspeed --include=localhost:1 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero3.json \\\n",
    "    --model_name_or_path fine-tuned_models/dbpedia/qald9plus-finetune \\\n",
    "    --do_train \\\n",
    "    --train_file  datasets/qald9plus/dbpedia/qald_9_plus_train_dbpedia.csv \\\n",
    "    --output_dir fine-tuned_models/dbpedia/qald9plus-finetune-new \\\n",
    "    --num_train_epochs 70 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 3000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name dbpedia-qald9plus-finetune-new \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GERBIL Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `eval.sh` to generate prediction files in QALD format and evaluate them with GERBIL.\n",
    "`eval.sh` is configured. \n",
    "\n",
    "```bash\n",
    "./eval.sh\n",
    "```\n",
    "\n",
    "Prediction files are stored in `pred_files/qald9plus-finetune`.\n",
    "The script uploads them to GERBIL along with the reference test file\n",
    "and waits for 5 minutes for the results.\n",
    "If the GERBIL experiment terminates, the results are stored in `pred_files/qald9plus-finetune/result.csv`, else, the experiment id is stored in this file. You can use the following commands to generate a csv files for results:\n",
    "\n",
    "```bash\n",
    "python3 code/gerbil_eval.py --experiment_id [experiment_id] --pred_path pred_files/qald9plus-finetune\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mst5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
