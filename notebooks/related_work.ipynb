{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Feature-based Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning applicable representations of words\n",
    "\n",
    "- non-neural\n",
    "  - Class-Based \\textit{n}-gram Models of Natural Language\n",
    "  - A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data\n",
    "  - Domain Adaptation with Structural Correspondence Learning\n",
    "- neural\n",
    "  - Distributed Representations of Words and Phrases and Their Compositionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised pre-training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- casual language modeling objective\n",
    "  - Semi-supervised sequence learning.\n",
    "  - Deep contextualized word representations\n",
    "  - Improving language understanding by generative pre-training\n",
    "  - Universal language model fine-tuning for text classification\n",
    "- denoising objectives (masked language modeling)\n",
    "  - BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Fine-tunning Approaches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "few parameters need to be learned from scratch\n",
    "\n",
    "- left-to-right language modeling\n",
    "- auto-encoder objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning from Supervised Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- natural language inference\n",
    "- machine translation\n",
    "- Computer vision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinguality in NLP Tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multilingual word embeddings\n",
    "  - Exploiting similarities among languages for machine translation\n",
    "  - Unsupervised multilingual word embeddings.\n",
    "  - Word transla- tion without parallel data\n",
    "- learning cross-lingual Models\n",
    "  - Unsupervised cross-lingual word embedding by multilingual neural language models\n",
    "  - Cross-lingual language model pretraining\n",
    "  - Unsupervised cross-lingual represen- tation learning at scale.\n",
    "- multilingual translation\n",
    "  - Multi-way, multilingual neural machine translation with a shared attention mechanism.\n",
    "  - Googleâ€™s multilingual neural machine translation system: Enabling zero-shot translation.\n",
    "  - Massively multilingual neural machine translation in the wild: Findings and challenges\n",
    "- on low-resource languages\n",
    "  - Universal neural machine translation for extremely low resource languages\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
