{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate test set with new answers\n",
    "\n",
    "In this notebook, we compare the test set of qald-9-plus which is avaliable online with the data set which we generated with new answers using the same SPARQL queries. \n",
    "\n",
    "Since the entities and relations in a knowledge graph vary over time, some SPARQLs may be not valid anymore. \n",
    "When we evaluate our model on the test set with old answers, it could happen that the model generates the same SPARQL as in test set, but doesn't get the correct answer. That would be a huge performance drop for our model. \n",
    "We retrieve the latest answer from DBpedia and Wikidata to make our evaluation more equitable. \n",
    "\n",
    "But, this approach also has a flaw. \n",
    "If our model predicts an incorrect SPARQL, from that we cannot retrieve any answer. \n",
    "And in the test set, it also doesn't get any answer in the current knowledge graph. \n",
    "Which means our model get an 1 for F1-score for this incorrect SPARQL. \n",
    "\n",
    "We compare the old and the new answers and summarize in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../code/')\n",
    "from dataset.Qald import Qald\n",
    "from utils.data_io import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_question_string(qald, index, language=\"en\"):\n",
    "    print(qald.entries[index].questions[language].question_string)\n",
    "\n",
    "def print_sparql(qald, index):\n",
    "    print(qald.entries[index].query.sparql)\n",
    "\n",
    "def get_answers(qald_entry):\n",
    "    try:\n",
    "        return qald_entry.answers[0][\"results\"][\"bindings\"]\n",
    "    except:\n",
    "        return qald_entry.answers[0][\"boolean\"]\n",
    "\n",
    "def print_answer(qald, index):\n",
    "    answers = get_answers(qald.entries[index])\n",
    "    print(answers)\n",
    "\n",
    "def are_answers_same(answers1, answers2):\n",
    "    if type(answers1)==type(answers2)==bool:\n",
    "        if answers1!=answers2:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    if len(answers1) != len(answers1):\n",
    "            return False\n",
    "    for binding in answers1:\n",
    "        if binding not in answers2:\n",
    "            return False\n",
    "    for binding in answers2:\n",
    "        if binding not in answers1:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def print_diff_answers(qald1, qald2, index):\n",
    "    answers_1 = get_answers(qald1.entries[index])\n",
    "    answers_2 = get_answers(qald2.entries[index])\n",
    "    if not are_answers_same(answers_1, answers_2):\n",
    "        print_question_string(qald1, index, \"en\")\n",
    "\n",
    "\n",
    "def compare_answers(qald1, qald2, index):\n",
    "    answers_1 = get_answers(qald1.entries[index]) \n",
    "    answers_2 = get_answers(qald2.entries[index])\n",
    "    if are_answers_same(answers_1, answers_2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def compare_empty_answers(qald1, qald2, index):\n",
    "    try:\n",
    "        bindings_1 = qald1.entries[index].answers[0][\"results\"][\"bindings\"]\n",
    "        bindings_2 = qald2.entries[index].answers[0][\"results\"][\"bindings\"]\n",
    "        if bindings_1 == bindings_2 == []:\n",
    "            return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def count_empty_bindings(qald):\n",
    "    number_of_entries = len(qald.entries)\n",
    "    select_question_counter = number_of_entries\n",
    "    empty_bindings_counter = 0\n",
    "    for index in range(number_of_entries):\n",
    "        try:\n",
    "            bindings = qald.entries[index].answers[0][\"results\"][\"bindings\"]\n",
    "        except:\n",
    "            bindings = []\n",
    "            select_question_counter -= 1\n",
    "            empty_bindings_counter -= 1\n",
    "        if not bindings:\n",
    "            empty_bindings_counter += 1\n",
    "    return empty_bindings_counter, select_question_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_json = read_json(\"../../datasets/qald9plus/dbpedia/qald_9_plus_test_dbpedia.json\")\n",
    "dbpedia = Qald(dbpedia_json, \"DBpedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_new_json = read_json(\"../../datasets/qald9plus/dbpedia/qald_9_plus_test_dbpedia-new.json\")\n",
    "dbpedia_new = Qald(dbpedia_new_json, \"DBpedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we show that we didn't change the SPARQLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the time zone of Salt Lake City?\n",
      "PREFIX res: <http://dbpedia.org/resource/> PREFIX dbp: <http://dbpedia.org/property/> SELECT DISTINCT ?uri WHERE { res:Salt_Lake_City <http://dbpedia.org/ontology/timeZone> ?uri }\n"
     ]
    }
   ],
   "source": [
    "print_question_string(dbpedia, 0, \"en\")\n",
    "print_sparql(dbpedia, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the time zone of Salt Lake City?\n",
      "PREFIX res: <http://dbpedia.org/resource/> PREFIX dbp: <http://dbpedia.org/property/> SELECT DISTINCT ?uri WHERE { res:Salt_Lake_City <http://dbpedia.org/ontology/timeZone> ?uri }\n"
     ]
    }
   ],
   "source": [
    "print_question_string(dbpedia_new, 0, \"en\")\n",
    "print_sparql(dbpedia_new, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compare the answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we show the answers of the first question \"What is the time zone of Salt Lake City?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uri': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Mountain_Time_Zone'}}]\n",
      "[{'uri': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Mountain_Time_Zone'}}]\n"
     ]
    }
   ],
   "source": [
    "print_answer(dbpedia, 0)\n",
    "print_answer(dbpedia_new, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many questions in our new generated test set have different answers than in the online version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 questions have the same answer as the online version.\n",
      "53 questions have different answers as the online version. \n"
     ]
    }
   ],
   "source": [
    "number_of_entries = len(dbpedia.entries)\n",
    "same_answers_counter, diff_answers_counter = 0, 0\n",
    "for index in range(number_of_entries):\n",
    "    if compare_answers(dbpedia, dbpedia_new, index):\n",
    "        same_answers_counter += 1\n",
    "    else:\n",
    "        diff_answers_counter += 1\n",
    "\n",
    "print(f\"{same_answers_counter} questions have the same answer as the online version.\")\n",
    "print(f\"{diff_answers_counter} questions have different answers as the online version. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many bindings in the online version of test set are empty. Here we exclude the \"ASK\" questions which have an answer true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the online version, 35 from 146 select questions have no answers\n"
     ]
    }
   ],
   "source": [
    "num_of_empty_bindings, num_of_select_questions = count_empty_bindings(dbpedia)\n",
    "print(f\"In the online version, {num_of_empty_bindings} from {num_of_select_questions} select questions have no answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how about our new version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our version, 35 from 146 select questions have no answers\n"
     ]
    }
   ],
   "source": [
    "num_of_empty_bindings, num_of_select_questions = count_empty_bindings(dbpedia_new)\n",
    "print(f\"In our version, {num_of_empty_bindings} from {num_of_select_questions} select questions have no answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these empty answers intersect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers of 29 questions are in both online and our version empty\n"
     ]
    }
   ],
   "source": [
    "same_empty_answers_counter = 0\n",
    "for index in range(number_of_entries):\n",
    "    if compare_empty_answers(dbpedia, dbpedia_new, index):\n",
    "        same_empty_answers_counter += 1\n",
    "\n",
    "print(f\"answers of {same_empty_answers_counter} questions are in both online and our version empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also checked the original [qald-9 data set](https://github.com/ag-sc/QALD/blob/master/9/data/qald-9-test-multilingual.json), \n",
    "all questions get answers, which means at the timepoint that qald-9 data set is created, these SPARQLs are all valid. \n",
    "\n",
    "But after a time, some properties have changed, these SPARQLs are now invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we look into these questions which have different answers in online and our version.\n",
    "\n",
    "We print out all questions with different answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who killed Caesar?\n",
      "Which American presidents were in office during the Vietnam War?\n",
      "Which artists were born on the same date as Rachel Stevens?\n",
      "Which European countries have a constitutional monarchy?\n",
      "Which countries have places with more than two caves?\n",
      "Which airports are located in California, USA?\n",
      "Which countries in the European Union adopted the Euro?\n",
      "Give me all professional skateboarders from Sweden.\n",
      "Give me all Argentine films.\n",
      "How did Michael Jackson die?\n",
      "Give me the homepage of Forbes.\n",
      "Give me all writers that won the Nobel Prize in literature.\n",
      "What is the nick name of Baghdad?\n",
      "In which city was the president of Montenegro born?\n",
      "What is the longest river in China?\n",
      "Which professional surfers were born in Australia?\n",
      "Give me all Dutch parties.\n",
      "Give me all animals that are extinct.\n",
      "Which German cities have more than 250000 inhabitants?\n",
      "How many students does the Free University of Amsterdam have?\n",
      "How many James Bond movies do exist?\n",
      "Who was Tom Hanks married to?\n",
      "Give me all cars that are produced in Germany.\n",
      "What is the highest volcano in Africa?\n",
      "Which poet wrote the most books?\n",
      "Give me all gangsters from the prohibition era.\n",
      "Show me all Czech movies.\n",
      "Give me all taikonauts.\n",
      "Which countries have more than ten volcanoes?\n",
      "Give me all movies with Tom Cruise.\n",
      "Who created English Wikipedia?\n",
      "Give me all female German chancellors.\n",
      "Which books were written by Danielle Steel?\n",
      "Who was influenced by Socrates?\n",
      "Which companies work in the aerospace industry as well as in medicine?\n",
      "Which languages are spoken in Estonia?\n",
      "Give me a list of all critically endangered birds.\n",
      "What languages are spoken in Pakistan?\n",
      "What is the wavelength of Indigo?\n",
      "Who was called Scarface?\n",
      "Which rivers flow into the North Sea?\n",
      "How many emperors did China have?\n",
      "How short is the shortest active NBA player?\n",
      "Which animals are critically endangered?\n",
      "Which politicians were married to a German?\n",
      "Which movies did Kurosawa direct?\n",
      "Give me all libraries established before 1400.\n",
      "Who founded Intel?\n",
      "Who is the youngest player in the Premier League?\n",
      "Which instruments does Cat Stevens play?\n",
      "Give me the capitals of all countries in Africa.\n",
      "Which bridges are of the same type as the Manhattan Bridge?\n",
      "Give me the websites of companies with more than 500000 employees.\n"
     ]
    }
   ],
   "source": [
    "for index in range(number_of_entries):\n",
    "    print_diff_answers(dbpedia, dbpedia_new, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the questions are fact that should not change, for example, \"Who killed Caesar?\" or \"Who founded Intel?\".\n",
    "However, the answers are different for the same SPARQL. \n",
    "It could be that there are some changes in the knowledge graph. \n",
    "\n",
    "On the other hand, there are some answers could vary over time. For example, \"Give me all movies with Tom Cruise.\" or \"Who is the youngest player in the Premier League?\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of questions that should be fact.\n",
    "\n",
    "- Who killed Caesar?\n",
    "- Which American presidents were in office during the Vietnam War?\n",
    "- What is the nick name of Baghdad?\n",
    "- What is the longest river in China?\n",
    "- Who created English Wikipedia?\n",
    "- What is the wavelength of Indigo?\n",
    "- Which movies did Kurosawa direct?\n",
    "- Give me all libraries established before 1400.\n",
    "- Who founded Intel?\n",
    "- Which instruments does Cat Stevens play?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out the reason, why they have different answers, we check the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- 97 questions have same answers\n",
    "- 53 questions have different answers\n",
    "    - 10 questions are facts and should not change its answers\n",
    "    - 43 questions have answers that can vary over time. \n",
    "- both online and our version have 35 empty answer bindings\n",
    "    - 29 of them intersect\n",
    "    - most of them are caused by changes in DBpedia. \n",
    "    - 6 answers become empty and 6 answers become not empty in our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_json = read_json(\"../../datasets/qald9plus/wikidata/qald_9_plus_test_wikidata.json\")\n",
    "wikidata = Qald(wikidata_json, \"Wikidata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_new_json = read_json(\"../../datasets/qald9plus/wikidata/qald_9_plus_test_wikidata_new.json\")\n",
    "wikidata_new = Qald(wikidata_new_json, \"Wikidata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Wikidata, we check the different answers first as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 questions have the same answer as the online version.\n",
      "54 questions have different answers as the online version. \n"
     ]
    }
   ],
   "source": [
    "number_of_entries = len(wikidata.entries)\n",
    "same_answers_counter, diff_answers_counter = 0, 0\n",
    "for index in range(number_of_entries):\n",
    "    if compare_answers(wikidata, wikidata_new, index):\n",
    "        same_answers_counter += 1\n",
    "    else:\n",
    "        diff_answers_counter += 1\n",
    "\n",
    "print(f\"{same_answers_counter} questions have the same answer as the online version.\")\n",
    "print(f\"{diff_answers_counter} questions have different answers as the online version. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which questions have different answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which artists were born on the same date as Rachel Stevens?\n",
      "What is the profession of Frank Herbert?\n",
      "Which European countries have a constitutional monarchy?\n",
      "Which countries have places with more than two caves?\n",
      "Who is the mayor of Berlin?\n",
      "Which countries in the European Union adopted the Euro?\n",
      "Which monarchs of the United Kingdom were married to a German?\n",
      "Give me all Argentine films.\n",
      "How did Michael Jackson die?\n",
      "Give me the homepage of Forbes.\n",
      "Which computer scientist won an oscar?\n",
      "Give me all writers that won the Nobel Prize in literature.\n",
      "How many scientists graduated from an Ivy League university?\n",
      "Which professional surfers were born in Australia?\n",
      "Give me all Dutch parties.\n",
      "How many moons does Mars have?\n",
      "Which space probes were sent into orbit around the sun?\n",
      "Which German cities have more than 250000 inhabitants?\n",
      "How many students does the Free University of Amsterdam have?\n",
      "What is the revenue of IBM?\n",
      "How many James Bond movies do exist?\n",
      "Give me all cars that are produced in Germany.\n",
      "Which poet wrote the most books?\n",
      "Give me all spacecrafts that flew to Mars.\n",
      "Show me all Czech movies.\n",
      "Give me all taikonauts.\n",
      "Which countries have more than ten volcanoes?\n",
      "Give me all movies with Tom Cruise.\n",
      "Give me all female German chancellors.\n",
      "Who owns Aldi?\n",
      "Which books were written by Danielle Steel?\n",
      "Which companies work in the aerospace industry as well as in medicine?\n",
      "Are there any castles in the United States?\n",
      "Give me a list of all critically endangered birds.\n",
      "Which countries are connected by the Rhine?\n",
      "Give me all chemical elements.\n",
      "Give me all American presidents of the last 20 years.\n",
      "What languages are spoken in Pakistan?\n",
      "Which rivers flow into the North Sea?\n",
      "Which daughters of British earls died at the same place they were born at?\n",
      "How many emperors did China have?\n",
      "What is the name of the university where Obama's wife studied?\n",
      "Which animals are critically endangered?\n",
      "Which politicians were married to a German?\n",
      "Give me all libraries established before 1400.\n",
      "Give me all Frisian islands that belong to the Netherlands.\n",
      "Which beer brewing companies are located in North-Rhine Westphalia?\n",
      "Who is the youngest player in the Premier League?\n",
      "Give me the capitals of all countries in Africa.\n",
      "Which bridges are of the same type as the Manhattan Bridge?\n",
      "How many companies were founded by the founder of Facebook?\n",
      "Which book has the most pages?\n",
      "Give me the websites of companies with more than 500000 employees.\n",
      "What were the names of the three ships by Columbus?\n"
     ]
    }
   ],
   "source": [
    "for index in range(number_of_entries):\n",
    "    print_diff_answers(wikidata, wikidata_new, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many answer bindings are empty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the online version, 6 from 133 select questions have no answers\n"
     ]
    }
   ],
   "source": [
    "num_of_empty_bindings, num_of_select_questions = count_empty_bindings(wikidata)\n",
    "print(f\"In the online version, {num_of_empty_bindings} from {num_of_select_questions} select questions have no answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the online version, 11 from 133 select questions have no answers\n"
     ]
    }
   ],
   "source": [
    "num_of_empty_bindings, num_of_select_questions = count_empty_bindings(wikidata_new)\n",
    "print(f\"In the online version, {num_of_empty_bindings} from {num_of_select_questions} select questions have no answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers of 5 questions are in both online and our version empty\n"
     ]
    }
   ],
   "source": [
    "same_empty_answers_counter = 0\n",
    "for index in range(number_of_entries):\n",
    "    if compare_empty_answers(wikidata, wikidata_new, index):\n",
    "        same_empty_answers_counter += 1\n",
    "\n",
    "print(f\"answers of {same_empty_answers_counter} questions are in both online and our version empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have empty answers in our version and online version\n",
    "- \"Butch Otter is the governor of which U.S. state?\"\n",
    "- \"Which airports are located in California, USA?\"\n",
    "- \"What is Elon Musk famous for?\"\n",
    "- \"Who does the voice of Bart Simpson?\"\n",
    "- \"Which professional surfers were born on the Philippines?\"\n",
    "\n",
    "There could be some logic error in these SPARQLs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Give me all female German chancellors.\" has answers in our test set but not in the online version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only have empty answers in our test set:\n",
    "- \"Which monarchs of the United Kingdom were married to a German?\"\n",
    "- \"Which books were written by Danielle Steel?\"\n",
    "- \"Which companies work in the aerospace industry as well as in medicine?\"\n",
    "- \"Which daughters of British earls died at the same place they were born at?\"\n",
    "- \"Which beer brewing companies are located in North-Rhine Westphalia?\"\n",
    "- \"What were the names of the three ships by Columbus?\"\n",
    "\n",
    "The reason could be changes in the Wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- 82 questions have same answers\n",
    "- 54 questions have different answers \n",
    "- 6 questions have empty answers in the online version\n",
    "- 11 questions have empty answers in our version\n",
    "    - 5 of them intersect\n",
    "    - 6 answers become empty\n",
    "    - 1 answer becomes not empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                         | DBpedia | Wikidata |\n",
    "|-------------------------|---------|----------|\n",
    "| same answers            | 97      | 82       |\n",
    "| diff answers            | 53      | 54       |\n",
    "| empty answers online    | 35      | 6        |\n",
    "| empty answers new       | 35      | 11       |\n",
    "| empty answers intersect | 29      | 5        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original qald-9 dataset, all questions have answers.\n",
    "In the qald-9-plus dataset, they use the same SPARQLs as in the qald-9, and many SPARQLs are already not valid for DBpedia.  \n",
    "Wikidata also has this problem, but since the SPARQLs for it are newer, most of them are still avaliable. \n",
    "\n",
    "Some of the changes in answers are caused by changes in the knowledge graph, which could make SPARQLs invalid. \n",
    "Other changes in answers are caused by the time. \n",
    "Many questions are time dependent, so it is reasonable to generate new answers for these SPARQLs. \n",
    "Since if the new generated prediction qald dataset would be compared to the old reference test set, it would get a low score even for the correct SPARQL, only because of the newer answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mengshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
