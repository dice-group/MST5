{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"SELECT var_uri WHERE bra_open var_uri wdt_P106 wd_Q3665646 ; wdt_P569 var_birthDate ; wdt_P166 var_award sep_dot var_award rdfs_label var_awardLabel FILTER (CONTAINS(STR( var_awardLabel), \"\"MVP\"\") sep_or sep_or CONTAINS(STR( var_awardLabel), \"\"Most Valuable Player\"\")) sep_dot FILTER (LANGMATCHES(LANG( var_awardLabel), \"\"EN\"\" ) ) bra_close ORDER BY DESC( var_birthDate) LIMIT 4\"\n",
    "\n",
    "example2 = \"ASK WHERE bra_open wd_Q9215 wdt_P26 var_o1 sep_dot bra_close\"\n",
    "\n",
    "example3 = \"SELECT DISTINCT var_uri WHERE bra_open var_uri wdt_P106 wd_Q18574233 ; wdt_P569 var_dateOfBirth sep_dot bra_close ORDER BY DESC( var_dateOfBirth) LIMIT 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test existing tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mt5 tokenizer\n",
    "\n",
    "the original mt5 tokenizer tokenizes words in SPARQL query very bad, \n",
    "e.g. \"DISTINCT\" is separated to  \"_D\", \"ISTI\" and \"NCT\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/m/mengshim/profiles/unix/cs/miniconda3/envs/mengshi/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mt5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'SELECT',\n",
       " '▁var',\n",
       " '_',\n",
       " 'uri',\n",
       " '▁W',\n",
       " 'HERE',\n",
       " '▁bra',\n",
       " '_',\n",
       " 'open',\n",
       " '▁var',\n",
       " '_',\n",
       " 'uri',\n",
       " '▁w',\n",
       " 'd',\n",
       " 't',\n",
       " '_',\n",
       " 'P',\n",
       " '106',\n",
       " '▁w',\n",
       " 'd',\n",
       " '_',\n",
       " 'Q',\n",
       " '3665',\n",
       " '646',\n",
       " '▁',\n",
       " ';',\n",
       " '▁w',\n",
       " 'd',\n",
       " 't',\n",
       " '_',\n",
       " 'P',\n",
       " '569',\n",
       " '▁var',\n",
       " '_',\n",
       " 'birth',\n",
       " 'Date',\n",
       " '▁',\n",
       " ';',\n",
       " '▁w',\n",
       " 'd',\n",
       " 't',\n",
       " '_',\n",
       " 'P',\n",
       " '166',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " '▁sep',\n",
       " '_',\n",
       " 'dot',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " '▁',\n",
       " 'rdf',\n",
       " 's',\n",
       " '_',\n",
       " 'label',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '▁FIL',\n",
       " 'TER',\n",
       " '▁(',\n",
       " 'CONTA',\n",
       " 'INS',\n",
       " '(',\n",
       " 'STR',\n",
       " '(',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '),',\n",
       " '▁',\n",
       " 'MVP',\n",
       " ')',\n",
       " '▁sep',\n",
       " '_',\n",
       " 'or',\n",
       " '▁sep',\n",
       " '_',\n",
       " 'or',\n",
       " '▁CONT',\n",
       " 'AINS',\n",
       " '(',\n",
       " 'STR',\n",
       " '(',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '),',\n",
       " '▁Most',\n",
       " '▁Valu',\n",
       " 'able',\n",
       " '▁Player',\n",
       " '))',\n",
       " '▁sep',\n",
       " '_',\n",
       " 'dot',\n",
       " '▁FIL',\n",
       " 'TER',\n",
       " '▁(',\n",
       " 'LANG',\n",
       " 'MATCH',\n",
       " 'ES',\n",
       " '(',\n",
       " 'LANG',\n",
       " '(',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '),',\n",
       " '▁EN',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " ')',\n",
       " '▁bra',\n",
       " '_',\n",
       " 'close',\n",
       " '▁',\n",
       " 'ORDER',\n",
       " '▁BY',\n",
       " '▁',\n",
       " 'DESC',\n",
       " '(',\n",
       " '▁var',\n",
       " '_',\n",
       " 'birth',\n",
       " 'Date',\n",
       " ')',\n",
       " '▁LIMIT',\n",
       " '▁4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = mt5_tokenizer.tokenize(example1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edrf model tokenizer\n",
    "\n",
    "tokenizer from edrf model performs slightly better since some tokens are added to tokenizer manually. But still it tokenizes other words badly, especially entity and relation tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'SELECT',\n",
       " '▁',\n",
       " 'var_uri',\n",
       " '▁W',\n",
       " 'HERE',\n",
       " '▁',\n",
       " 'bra_open',\n",
       " '▁',\n",
       " 'var_uri',\n",
       " '▁w',\n",
       " 'd',\n",
       " 't',\n",
       " '_',\n",
       " 'P',\n",
       " '106',\n",
       " '▁w',\n",
       " 'd',\n",
       " '_',\n",
       " 'Q',\n",
       " '3665',\n",
       " '646',\n",
       " '▁',\n",
       " ';',\n",
       " '▁w',\n",
       " 'd',\n",
       " 't',\n",
       " '_',\n",
       " 'P',\n",
       " '569',\n",
       " '▁var',\n",
       " '_',\n",
       " 'birth',\n",
       " 'Date',\n",
       " '▁',\n",
       " ';',\n",
       " '▁w',\n",
       " 'd',\n",
       " 't',\n",
       " '_',\n",
       " 'P',\n",
       " '166',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " '▁',\n",
       " 'sep_dot',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " '▁',\n",
       " 'rdf',\n",
       " 's',\n",
       " '_',\n",
       " 'label',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '▁FIL',\n",
       " 'TER',\n",
       " '▁(',\n",
       " 'CONTA',\n",
       " 'INS',\n",
       " '(',\n",
       " 'STR',\n",
       " '(',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '),',\n",
       " '▁',\n",
       " 'MVP',\n",
       " ')',\n",
       " '▁',\n",
       " 'sep_or',\n",
       " '▁',\n",
       " 'sep_or',\n",
       " '▁CONT',\n",
       " 'AINS',\n",
       " '(',\n",
       " 'STR',\n",
       " '(',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '),',\n",
       " '▁Most',\n",
       " '▁Valu',\n",
       " 'able',\n",
       " '▁Player',\n",
       " '))',\n",
       " '▁',\n",
       " 'sep_dot',\n",
       " '▁FIL',\n",
       " 'TER',\n",
       " '▁(',\n",
       " 'LANG',\n",
       " 'MATCH',\n",
       " 'ES',\n",
       " '(',\n",
       " 'LANG',\n",
       " '(',\n",
       " '▁var',\n",
       " '_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '),',\n",
       " '▁EN',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " 'bra_close',\n",
       " '▁',\n",
       " 'ORDER',\n",
       " '▁BY',\n",
       " '▁',\n",
       " 'DESC',\n",
       " '(',\n",
       " '▁var',\n",
       " '_',\n",
       " 'birth',\n",
       " 'Date',\n",
       " ')',\n",
       " '▁LIMIT',\n",
       " '▁4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edrf_tokenizer = AutoTokenizer.from_pretrained(\"en_de_ru_fr\")\n",
    "tokens = edrf_tokenizer.tokenize(example1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a tokenizer with SPARQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      SELECT var_uri WHERE bra_open var_uri wdt_P31 ...\n",
       "1      SELECT var_uri WHERE bra_open wd_Q40984 wdt_P1...\n",
       "2      SELECT var_uri WHERE bra_open var_uri wdt_P19 ...\n",
       "3      SELECT DISTINCT var_uri WHERE bra_open wd_Q177...\n",
       "4      SELECT DISTINCT var_uri WHERE bra_open wd_Q60 ...\n",
       "                             ...                        \n",
       "366     SELECT var_s1 WHERE bra_open var_s1 wdt_P159 ...\n",
       "367    SELECT DISTINCT var_uri WHERE bra_open var_uri...\n",
       "368    SELECT DISTINCT var_s1 WHERE bra_open var_s1 w...\n",
       "369    SELECT var_o1 WHERE bra_open wd_Q858840 wdt_P1...\n",
       "370    SELECT var_o1 WHERE bra_open wd_Q339 wdt_P61 v...\n",
       "Name: query, Length: 371, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"wikidata_en.csv\")\n",
    "dataset['query']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = mt5_tokenizer.train_new_from_iterator(dataset['query'], 260000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'SE',\n",
       " 'L',\n",
       " 'E',\n",
       " 'CT',\n",
       " '▁var_uri',\n",
       " '▁',\n",
       " 'W',\n",
       " 'H',\n",
       " 'ER',\n",
       " 'E',\n",
       " '▁bra_',\n",
       " 'op',\n",
       " 'en',\n",
       " '▁var_uri',\n",
       " '▁wdt_P106',\n",
       " '▁wd_Q36',\n",
       " '6',\n",
       " '5646',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P5',\n",
       " '69',\n",
       " '▁var_birthDate',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P16',\n",
       " '6',\n",
       " '▁var_award',\n",
       " '▁sep_',\n",
       " 'd',\n",
       " 'ot',\n",
       " '▁var_award',\n",
       " '▁rdfs_label',\n",
       " '▁var_awardLabel',\n",
       " '▁FILTER',\n",
       " '▁',\n",
       " '(CONTAINS(',\n",
       " 'ST',\n",
       " 'R(',\n",
       " '▁var_awardLabel',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'M',\n",
       " 'V',\n",
       " 'P',\n",
       " ')',\n",
       " '▁sep_',\n",
       " 'or',\n",
       " '▁sep_',\n",
       " 'or',\n",
       " '▁',\n",
       " 'CONTAINS(',\n",
       " 'ST',\n",
       " 'R(',\n",
       " '▁var_awardLabel',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'M',\n",
       " 'o',\n",
       " 'st',\n",
       " '▁V',\n",
       " 'alu',\n",
       " 'ab',\n",
       " 'le',\n",
       " '▁',\n",
       " 'Pla',\n",
       " 'y',\n",
       " 'er',\n",
       " '))',\n",
       " '▁sep_',\n",
       " 'd',\n",
       " 'ot',\n",
       " '▁FILTER',\n",
       " '▁',\n",
       " '(LANG',\n",
       " 'M',\n",
       " 'AT',\n",
       " 'C',\n",
       " 'HE',\n",
       " 'S',\n",
       " '(LANG',\n",
       " '(',\n",
       " '▁var_awardLabel',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'E',\n",
       " 'N',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " ')',\n",
       " '▁bra_',\n",
       " 'close',\n",
       " '▁O',\n",
       " 'R',\n",
       " 'D',\n",
       " 'ER',\n",
       " '▁B',\n",
       " 'Y',\n",
       " '▁DESC(',\n",
       " '▁var_birthDate',\n",
       " ')',\n",
       " '▁LIMI',\n",
       " 'T',\n",
       " '▁',\n",
       " '4']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained(\"sparql-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_edrf_tokenizer = edrf_tokenizer.train_new_from_iterator(dataset['query'], 260000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'SE',\n",
       " 'L',\n",
       " 'E',\n",
       " 'CT',\n",
       " '▁',\n",
       " 'var_uri',\n",
       " '▁',\n",
       " 'W',\n",
       " 'H',\n",
       " 'ER',\n",
       " 'E',\n",
       " '▁',\n",
       " 'bra_open',\n",
       " '▁',\n",
       " 'var_uri',\n",
       " '▁wdt_P106',\n",
       " '▁wd_Q36',\n",
       " '6',\n",
       " '5646',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P5',\n",
       " '69',\n",
       " '▁var_birthDate',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P16',\n",
       " '6',\n",
       " '▁var_award',\n",
       " '▁',\n",
       " 'sep_dot',\n",
       " '▁var_award',\n",
       " '▁rdfs_label',\n",
       " '▁var_awardLabel',\n",
       " '▁FILTER',\n",
       " '▁',\n",
       " '(CONTAINS(',\n",
       " 'ST',\n",
       " 'R(',\n",
       " '▁var_awardLabel',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'M',\n",
       " 'V',\n",
       " 'P',\n",
       " ')',\n",
       " '▁',\n",
       " 'sep_or',\n",
       " '▁',\n",
       " 'sep_or',\n",
       " '▁',\n",
       " 'CONTAINS(',\n",
       " 'ST',\n",
       " 'R(',\n",
       " '▁var_awardLabel',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'M',\n",
       " 'o',\n",
       " 'st',\n",
       " '▁V',\n",
       " 'alu',\n",
       " 'ab',\n",
       " 'le',\n",
       " '▁',\n",
       " 'Pla',\n",
       " 'y',\n",
       " 'er',\n",
       " '))',\n",
       " '▁',\n",
       " 'sep_dot',\n",
       " '▁FILTER',\n",
       " '▁',\n",
       " '(LANG',\n",
       " 'M',\n",
       " 'AT',\n",
       " 'C',\n",
       " 'HE',\n",
       " 'S',\n",
       " '(LANG',\n",
       " '(',\n",
       " '▁var_awardLabel',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'E',\n",
       " 'N',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " 'bra_close',\n",
       " '▁O',\n",
       " 'R',\n",
       " 'D',\n",
       " 'ER',\n",
       " '▁B',\n",
       " 'Y',\n",
       " '▁DESC(',\n",
       " '▁var_birthDate',\n",
       " ')',\n",
       " '▁LIMI',\n",
       " 'T',\n",
       " '▁',\n",
       " '4']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_edrf_tokenizer.tokenize(example1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- special tokens like \"SELECT\", \"WHERE\" are still separated. They need to be added into tokenizer\n",
    "\n",
    "- entity and relations are also separated e.g. \"_wd_Q36\", \"6\", \"5646\". This issue has to be solved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer with SPARQL queries from Wikidata SPARQL Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess SPARQL queries from Wikidata SPARQL Logs\n",
    "\n",
    "Wikidata SPARQL Logs collect SPARQL queries in different time intervals, which can be used to train tokenizer. Queries are URL-encoded, thus queries must be decoded and transformed to the same format as our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import unquote\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SELECT+*%0AWHERE+%7B%0A++%3Fvar1++%3Chttp%3A%2F%2Fwww.wikidata.org%2Fprop%2Fdirect%2FP698%3E++%2229066813%22.%0A%7D%0A\\t2018-02-26 00:00:00\\trobotic\\tPBB_core fastrun']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(query_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anonymizedQuery</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sourceCategory</th>\n",
       "      <th>user_agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT+%3Fvar1++%3Fvar1Label+%28+COUNT+%28+DIS...</td>\n",
       "      <td>2018-02-26 00:00:08</td>\n",
       "      <td>organic</td>\n",
       "      <td>browser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT+DISTINCT+%3Fvar1+%0AWHERE+%7B%0A++%3Fva...</td>\n",
       "      <td>2018-02-26 00:00:27</td>\n",
       "      <td>organic</td>\n",
       "      <td>browser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT+%3Fvar1++%3Fvar1Label+%28+COUNT+%28+DIS...</td>\n",
       "      <td>2018-02-26 00:00:31</td>\n",
       "      <td>organic</td>\n",
       "      <td>browser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT+DISTINCT+%3Fvar1++%3Fvar1Label++%3Fvar2...</td>\n",
       "      <td>2018-02-26 00:00:43</td>\n",
       "      <td>organic</td>\n",
       "      <td>browser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT+%3Fvar1++%3Fvar2Label++%3Fvar3+%0AWHERE...</td>\n",
       "      <td>2018-02-26 00:00:57</td>\n",
       "      <td>organic</td>\n",
       "      <td>browser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     anonymizedQuery            timestamp  \\\n",
       "0  SELECT+%3Fvar1++%3Fvar1Label+%28+COUNT+%28+DIS...  2018-02-26 00:00:08   \n",
       "1  SELECT+DISTINCT+%3Fvar1+%0AWHERE+%7B%0A++%3Fva...  2018-02-26 00:00:27   \n",
       "2  SELECT+%3Fvar1++%3Fvar1Label+%28+COUNT+%28+DIS...  2018-02-26 00:00:31   \n",
       "3  SELECT+DISTINCT+%3Fvar1++%3Fvar1Label++%3Fvar2...  2018-02-26 00:00:43   \n",
       "4  SELECT+%3Fvar1++%3Fvar2Label++%3Fvar3+%0AWHERE...  2018-02-26 00:00:57   \n",
       "\n",
       "  sourceCategory user_agent  \n",
       "0        organic    browser  \n",
       "1        organic    browser  \n",
       "2        organic    browser  \n",
       "3        organic    browser  \n",
       "4        organic    browser  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('2018-02-26_2018-03-25_organic.tsv', sep='\\t', header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    SELECT+%3Fvar1++%3Fvar1Label+%28+COUNT+%28+DIS...\n",
       "1    SELECT+DISTINCT+%3Fvar1+%0AWHERE+%7B%0A++%3Fva...\n",
       "2    SELECT+%3Fvar1++%3Fvar1Label+%28+COUNT+%28+DIS...\n",
       "3    SELECT+DISTINCT+%3Fvar1++%3Fvar1Label++%3Fvar2...\n",
       "4    SELECT+%3Fvar1++%3Fvar2Label++%3Fvar3+%0AWHERE...\n",
       "Name: anonymizedQuery, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data = data[\"anonymizedQuery\"]\n",
    "query_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_pattern = [\n",
    "    [r'<http://dbpedia.org/resource/(.*?)>\\.?', 'dbr:'],\n",
    "    [r'<http://dbpedia.org/property/(.*?)>\\.?', 'dbp:'],\n",
    "    [r'<http://dbpedia.org/ontology/(.*?)>\\.?', 'dbo:'],\n",
    "    [r'<http://dbpedia.org/class/yago/(.*?)>\\.?', 'yago:'],\n",
    "    [r'onto:(.*)', 'dbo:'],\n",
    "    [r'<http://www.wikidata.org/prop/direct/(.*?)>', 'wdt:'],\n",
    "    [r'<http://www.wikidata.org/entity/(.*?)>', 'wd:'],\n",
    "    [r'http://www.wikidata.org/prop/(.*?)', 'p:'],\n",
    "    [r'<http://www.w3.org/2000/01/rdf-schema#(.*?)', 'rdfs:'],\n",
    "    [r'<http://wikiba.se/ontology#(.*?)>', 'wbo:'],\n",
    "    [r'<http://www.bigdata.com/queryHints#(.*?)>', 'bdqh:'],\n",
    "    [r'<http://schema.org/(.*?)>', 'schema:'],\n",
    "    [r'<http://www.opengis.net/ont/geosparql#(.*?)>', 'og:'],\n",
    "    [r'<https://(.*?).wikipedia.org/>', 'wiki:']\n",
    "]\n",
    "\n",
    "replacement = [\n",
    "    ['<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>', 'rdf:type'],\n",
    "    ['<http://www.bigdata.com/rdf#serviceParam>', 'rdf:service'],\n",
    "    ['{', ' bra_open '],\n",
    "    ['}', ' bra_close '],\n",
    "    ['?', ' var_'],\n",
    "    [':', '_'],\n",
    "    ['.', ' sep_dot '],\n",
    "    ['|', ' sep_or '],\n",
    "    [\"+\", \" \"],\n",
    "    [\"\\n\", \" \"]\n",
    "]\n",
    "\n",
    "def process_query(query):\n",
    "    query = unquote(query)\n",
    "    for pattern in prefix_pattern:\n",
    "        query = re.sub(pattern[0], pattern[1]+r'\\1', query)\n",
    "    for replace in replacement:\n",
    "        query = query.replace(replace[0], replace[1])\n",
    "    query = re.sub(' +', ' ', query)\n",
    "    return query\n",
    "\n",
    "process_query_generator = (process_query(query) for query in query_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_generator():\n",
    "    with open(\"2018-02-26_2018-03-25_all.tsv\", \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        next(reader, None)  # skip the headers\n",
    "        for row in reader:\n",
    "            # process each row\n",
    "            yield process_query(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT * WHERE bra_open var_var1 wdt_P698 \"29066813\" sep_dot bra_close '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(query_generator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/m/mengshim/profiles/unix/cs/miniconda3/envs/mengshi/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(process_query_generator, 300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_trained_tokenizer = mt5_tokenizer.train_new_from_iterator(query_generator(), 400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sparql-tokenizer-trainedon_all/tokenizer_config.json',\n",
       " 'sparql-tokenizer-trainedon_all/special_tokens_map.json',\n",
       " 'sparql-tokenizer-trainedon_all/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trained_tokenizer.save_pretrained(\"sparql-tokenizer-trainedon_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'S',\n",
       " 'ELE',\n",
       " 'CT',\n",
       " '▁var_',\n",
       " 'uri',\n",
       " '▁',\n",
       " 'WH',\n",
       " 'ER',\n",
       " 'E',\n",
       " '▁',\n",
       " 'bra',\n",
       " '_',\n",
       " 'open',\n",
       " '▁var_',\n",
       " 'uri',\n",
       " '▁wdt_P10',\n",
       " '6',\n",
       " '▁wd_Q3665',\n",
       " '646',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P',\n",
       " '569',\n",
       " '▁var_',\n",
       " 'birth',\n",
       " 'D',\n",
       " 'ate',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P16',\n",
       " '6',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'ward',\n",
       " '▁',\n",
       " 'sep',\n",
       " '_',\n",
       " 'dot',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'ward',\n",
       " '▁rdf',\n",
       " 's_la',\n",
       " 'bel',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " '▁',\n",
       " 'FI',\n",
       " 'L',\n",
       " 'T',\n",
       " 'ER',\n",
       " '▁',\n",
       " '(',\n",
       " 'C',\n",
       " 'ON',\n",
       " 'T',\n",
       " 'A',\n",
       " 'IN',\n",
       " 'S',\n",
       " '(',\n",
       " 'S',\n",
       " 'T',\n",
       " 'R',\n",
       " '(',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'M',\n",
       " 'V',\n",
       " 'P',\n",
       " ')',\n",
       " '▁',\n",
       " 'sep',\n",
       " '_or',\n",
       " '▁',\n",
       " 'sep',\n",
       " '_or',\n",
       " '▁',\n",
       " 'C',\n",
       " 'ON',\n",
       " 'T',\n",
       " 'A',\n",
       " 'IN',\n",
       " 'S',\n",
       " '(',\n",
       " 'S',\n",
       " 'T',\n",
       " 'R',\n",
       " '(',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'Most',\n",
       " '▁',\n",
       " 'V',\n",
       " 'alu',\n",
       " 'able',\n",
       " '▁',\n",
       " 'Player',\n",
       " ')',\n",
       " ')',\n",
       " '▁',\n",
       " 'sep',\n",
       " '_',\n",
       " 'dot',\n",
       " '▁',\n",
       " 'FI',\n",
       " 'L',\n",
       " 'T',\n",
       " 'ER',\n",
       " '▁',\n",
       " '(',\n",
       " 'LANG',\n",
       " 'MA',\n",
       " 'T',\n",
       " 'C',\n",
       " 'HE',\n",
       " 'S',\n",
       " '(',\n",
       " 'LANG',\n",
       " '(',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'ward',\n",
       " 'Label',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'E',\n",
       " 'N',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " 'bra',\n",
       " '_c',\n",
       " 'lose',\n",
       " '▁',\n",
       " 'O',\n",
       " 'R',\n",
       " 'D',\n",
       " 'ER',\n",
       " '▁',\n",
       " 'BY',\n",
       " '▁',\n",
       " 'DE',\n",
       " 'S',\n",
       " 'C',\n",
       " '(',\n",
       " '▁var_',\n",
       " 'birth',\n",
       " 'D',\n",
       " 'ate',\n",
       " ')',\n",
       " '▁',\n",
       " 'L',\n",
       " 'I',\n",
       " 'MIT',\n",
       " '▁',\n",
       " '4']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trained_tokenizer.tokenize(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'A',\n",
       " 'SK',\n",
       " '▁',\n",
       " 'WH',\n",
       " 'ER',\n",
       " 'E',\n",
       " '▁',\n",
       " 'bra',\n",
       " '_',\n",
       " 'open',\n",
       " '▁wd_Q9215',\n",
       " '▁wdt_P26',\n",
       " '▁var_o',\n",
       " '1',\n",
       " '▁',\n",
       " 'sep',\n",
       " '_',\n",
       " 'dot',\n",
       " '▁',\n",
       " 'bra',\n",
       " '_c',\n",
       " 'lose']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trained_tokenizer.tokenize(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'S',\n",
       " 'ELE',\n",
       " 'CT',\n",
       " '▁',\n",
       " 'DI',\n",
       " 'S',\n",
       " 'T',\n",
       " 'IN',\n",
       " 'CT',\n",
       " '▁var_',\n",
       " 'uri',\n",
       " '▁',\n",
       " 'WH',\n",
       " 'ER',\n",
       " 'E',\n",
       " '▁',\n",
       " 'bra',\n",
       " '_',\n",
       " 'open',\n",
       " '▁var_',\n",
       " 'uri',\n",
       " '▁wdt_P10',\n",
       " '6',\n",
       " '▁wd_Q18574',\n",
       " '233',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P',\n",
       " '569',\n",
       " '▁var_',\n",
       " 'date',\n",
       " 'Of',\n",
       " 'Birth',\n",
       " '▁',\n",
       " 'sep',\n",
       " '_',\n",
       " 'dot',\n",
       " '▁',\n",
       " 'bra',\n",
       " '_c',\n",
       " 'lose',\n",
       " '▁',\n",
       " 'O',\n",
       " 'R',\n",
       " 'D',\n",
       " 'ER',\n",
       " '▁',\n",
       " 'BY',\n",
       " '▁',\n",
       " 'DE',\n",
       " 'S',\n",
       " 'C',\n",
       " '(',\n",
       " '▁var_',\n",
       " 'date',\n",
       " 'Of',\n",
       " 'Birth',\n",
       " ')',\n",
       " '▁',\n",
       " 'L',\n",
       " 'I',\n",
       " 'MIT',\n",
       " '▁',\n",
       " '1']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trained_tokenizer.tokenize(example3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁SELECT',\n",
       " '▁var_',\n",
       " 'uri',\n",
       " '▁WH',\n",
       " 'ERE',\n",
       " '▁bra_',\n",
       " 'open',\n",
       " '▁var_',\n",
       " 'uri',\n",
       " '▁wdt_P106',\n",
       " '▁wd_Q36',\n",
       " '65646',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P569',\n",
       " '▁var_',\n",
       " 'birthDate',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P166',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'w',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " '▁sep_',\n",
       " 'dot',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'w',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " '▁rdfs_',\n",
       " 'label',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'w',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " 'Label',\n",
       " '▁FIL',\n",
       " 'TER',\n",
       " '▁',\n",
       " '(',\n",
       " 'CONT',\n",
       " 'AIN',\n",
       " 'S',\n",
       " '(',\n",
       " 'STR',\n",
       " '(',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'w',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " 'Label',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'M',\n",
       " 'V',\n",
       " 'P',\n",
       " ')',\n",
       " '▁sep_',\n",
       " 'o',\n",
       " 'r',\n",
       " '▁sep_',\n",
       " 'o',\n",
       " 'r',\n",
       " '▁CONT',\n",
       " 'AIN',\n",
       " 'S',\n",
       " '(',\n",
       " 'STR',\n",
       " '(',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'w',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " 'Label',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'M',\n",
       " 'o',\n",
       " 's',\n",
       " 't',\n",
       " '▁',\n",
       " 'V',\n",
       " 'a',\n",
       " 'l',\n",
       " 'u',\n",
       " 'a',\n",
       " 'b',\n",
       " 'l',\n",
       " 'e',\n",
       " '▁',\n",
       " 'P',\n",
       " 'l',\n",
       " 'ayer',\n",
       " ')',\n",
       " ')',\n",
       " '▁sep_',\n",
       " 'dot',\n",
       " '▁FIL',\n",
       " 'TER',\n",
       " '▁',\n",
       " '(',\n",
       " 'LANG',\n",
       " 'MAT',\n",
       " 'CHE',\n",
       " 'S',\n",
       " '(',\n",
       " 'LANG',\n",
       " '(',\n",
       " '▁var_',\n",
       " 'a',\n",
       " 'w',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " 'Label',\n",
       " ')',\n",
       " ',',\n",
       " '▁',\n",
       " 'E',\n",
       " 'N',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " ')',\n",
       " '▁bra_',\n",
       " 'close',\n",
       " '▁',\n",
       " 'ORD',\n",
       " 'ER',\n",
       " '▁',\n",
       " 'BY',\n",
       " '▁DESC(',\n",
       " '▁var_',\n",
       " 'birthDate',\n",
       " ')',\n",
       " '▁LI',\n",
       " 'MIT',\n",
       " '▁',\n",
       " '4']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'ASK',\n",
       " '▁WH',\n",
       " 'ERE',\n",
       " '▁bra_',\n",
       " 'open',\n",
       " '▁wd_Q9215',\n",
       " '▁wdt_P26',\n",
       " '▁var_',\n",
       " 'o',\n",
       " '1',\n",
       " '▁sep_',\n",
       " 'dot',\n",
       " '▁bra_',\n",
       " 'close']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁SELECT',\n",
       " '▁DISTINCT',\n",
       " '▁var_',\n",
       " 'uri',\n",
       " '▁WH',\n",
       " 'ERE',\n",
       " '▁bra_',\n",
       " 'open',\n",
       " '▁var_',\n",
       " 'uri',\n",
       " '▁wdt_P106',\n",
       " '▁wd_Q18',\n",
       " '57423',\n",
       " '3',\n",
       " '▁',\n",
       " ';',\n",
       " '▁wdt_P569',\n",
       " '▁var_',\n",
       " 'd',\n",
       " 'ate',\n",
       " 'O',\n",
       " 'f',\n",
       " 'B',\n",
       " 'i',\n",
       " 'rth',\n",
       " '▁sep_',\n",
       " 'dot',\n",
       " '▁bra_',\n",
       " 'close',\n",
       " '▁',\n",
       " 'ORD',\n",
       " 'ER',\n",
       " '▁',\n",
       " 'BY',\n",
       " '▁DESC(',\n",
       " '▁var_',\n",
       " 'd',\n",
       " 'ate',\n",
       " 'O',\n",
       " 'f',\n",
       " 'B',\n",
       " 'i',\n",
       " 'rth',\n",
       " ')',\n",
       " '▁LI',\n",
       " 'MIT',\n",
       " '▁',\n",
       " '1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"sparql-tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03.15.2023\n",
    "\n",
    "add tokens to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/m/mengshim/profiles/unix/cs/miniconda3/envs/mengshi/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mt5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     select distinct var_obj where bra_open wd_Q18...\n",
       "1    SELECT var_answer WHERE bra_open wd_Q169794 wd...\n",
       "2    ASK WHERE bra_open wd_Q174843 wdt_P106 wd_Q180...\n",
       "3    SELECT var_answer WHERE bra_open wd_Q675176 wd...\n",
       "4    select distinct var_answer where bra_open wd_Q...\n",
       "Name: query, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcquad = pd.read_csv(\"datasets/lcqald_wikidata.csv\")\n",
    "lcquad[\"query\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mengshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "831f47b340cd9142d044ce7606dedd31def08d9bf4dbb095e8657e50ac098d96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
