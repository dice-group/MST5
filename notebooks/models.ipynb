{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Transformers\n",
    "\n",
    "[Installation](https://huggingface.co/docs/transformers/installation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bidirectional Encoder Representations from Transformers\n",
    "- Transformer encoder: bidirectional transformer\n",
    "- Bidirectioal: each word can see itself indirectly based on left and right context\n",
    "- self-attention without mask\n",
    "- training:\n",
    "  - mask words\n",
    "  - next sentence prediction\n",
    "\n",
    "Input/Output:\n",
    "- single sentence/pair of sentences\n",
    "- token Embeddings(WordPiece) + Segment Embeddings + Position Embeddings\n",
    "- Softmax output layer for token-level tasks\n",
    "- [CLS] for classification\n",
    "- SQuAD v1.1: introduce start and end vector during fine-tuning\n",
    "- SQuAD v2.0: start and end at [CLS]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multilingual sequence-to-sequence denoising auto-encoder\n",
    "- encoder only\n",
    "- training:\n",
    "  - mask words\n",
    "  - next sentence prediction\n",
    "- 104 languages\n",
    "- Zero shot\n",
    "  - massively multi-lingual MT (Johnson et al.,2017; Gu et al., 2019)\n",
    "  - distillation through pivoting (Chen et al., 2017)\n",
    "- Unsupervised Machine Translation\n",
    "  - Back-Translation\n",
    "  - Language Transfer\n",
    "  - Combined"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model with sufficient capacity will begin to learn to infer and perform the tasks demostrated in natural language sequences in order to better predict them, regardless of their method of procurement.\n",
    "\n",
    "- WebText\n",
    "- Transformer based\n",
    "- OpenAI GPT with a few modifications\n",
    "- unsupervised\n",
    "- multitask\n",
    "- masked self-attention\n",
    "\n",
    "Input/Output(GPT):\n",
    "- input: text + position embedding\n",
    "- predict vector with decoder layers and sample a word with top-k sampling\n",
    "- use decoder to predict next token in language modeling\n",
    "- start, delimiter($) and end tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5: Text-To-Text Transfer Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[huggingface t5](https://huggingface.co/docs/transformers/model_doc/t5)\n",
    "\n",
    "- Architecture -> Encoder-Decoder\n",
    "  - Encoder-Decoder ✅\n",
    "    - Encoder: fully-visible attention mask\n",
    "    - Decoder: causal masking pattern\n",
    "  - Language model (decoder-only)\n",
    "  - Prefix LM (decoder-only)\n",
    "- Unsupervised objectives\n",
    "  - High-level approaches\n",
    "    - BERT-style ✅\n",
    "    - Language modeling\n",
    "    - Deshuffling\n",
    "  - Corruption strategies\n",
    "    - Replace spans ✅\n",
    "    - Mask\n",
    "    - Drop\n",
    "  - Corruption rate\n",
    "    - 10%\n",
    "    - 15% ✅\n",
    "    - 25%\n",
    "    - 50%\n",
    "  - Corrupted span length\n",
    "    - i.i.d ✅\n",
    "    - 2\n",
    "    - 3\n",
    "    - 5\n",
    "    - 10\n",
    "- pre-trained on C4\n",
    "- allows the use of exactly the same training objective (teacher-forced maximum- likelihood) for every task\n",
    "- computational effort\n",
    "\n",
    "Input/Output:\n",
    "- sequence-to-sequence\n",
    "- task-specific prefix in original input sequence\n",
    "- sequence of input tokens are mapped to a sequence of output embedding\n",
    "- output is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix\n",
    "- text embedding + position embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5: multilingual pre-trained text-to-text transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[huggingface mt5](https://huggingface.co/docs/transformers/model_doc/mt5)\n",
    "\n",
    "- pre-trained on mC4\n",
    "- 101 languages\n",
    "- pre-trained unsupervisedly -> no real advantage to using a task prefix during single-task fine-tunning\n",
    "\n",
    "Zero-Shot Generation\n",
    "\n",
    "- Domain Preserving Training\n",
    "- Illegal Predictions\n",
    "  - Normalization: most mT5-XXL illegal predictions are resolved by normalization\n",
    "  - Grammatical adjustment\n",
    "  - Accidental translation\n",
    "    - don't use English only for fine-tuning\n",
    "    - reduce the language sampling parameter $\\alpha$ (e.g. to 0.1)\n",
    "\n",
    "Q: fine-tune on an English generative task, then it works for all other lanugages?\n",
    "\n",
    "A: not 100% reliable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT-based (encoder)\n",
    "  - Casual Language Modeling (CLM)\n",
    "  - Masked Language Modeling (MLM)\n",
    "  - Translation Language Modeling (TLM)\n",
    "- cross-lingual pre-training objectives\n",
    "- reduce perplexity\n",
    "- 100 languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence piece model\n",
    "- longer training time\n",
    "- larger model\n",
    "- outperforms monolingual BERT by making use of multilingual training\n",
    "- used CommonCrawl data instead of Wikipedia\n",
    "- trained with a cross-lingual masked language modeling objective\n",
    "- analysis the trade-offs and limitations of multilingual language models\n",
    "- 100 languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- combined BERT and GPT\n",
    "- mask and reconstruct sentences\n",
    "- an additional encoder for machine translation before encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- document-level sequence-to-sequence\n",
    "- encoder-decoder\n",
    "- Encoder: retrieve related texts in many languages\n",
    "- Decoder: reconstruct the original text from retrieved texts\n",
    "- reconstruct a document in one language by retrieving documents in other languages\n",
    "- performs well even merely pre-training\n",
    "- 26 languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RemBert (Rebalanced multilingual BERT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- re-evaluate the standard practice of sharing weights between input and output embeddings\n",
    "- decoupled embeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models\n",
    "- reallocate the input embedding parameters\n",
    "- allocate additional capacity to the output embedding provides benefits to the model that persist through the fine-tuning stage even though the output embedding is discarded after pre-training\n",
    "- larger output embeddings prevent the model's last layers from overspecializaing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
