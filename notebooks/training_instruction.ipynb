{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions to train the model\n",
    "\n",
    "#### **Model attributes**\n",
    "- uses Wikidata knowledge graph\n",
    "- uses (pre-trained) mT5-XL as base model\n",
    "- pre-trained on LC-QuAD 2.0 for 15 epochs\n",
    "- fine-tuned on QALD-9-Plus (custom) on all languages for 15 epochs\n",
    "- utilizes linguistic context, entity knowledge and padding in pre-training and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note: Please run all commands in the root of this repo__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pre-requisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install requirements**\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download/Install SpaCy models**\n",
    "\n",
    "```bash\n",
    "pip install -U pip setuptools wheel\n",
    "pip install -U spacy\n",
    "python -m spacy download zh_core_web_sm\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download fr_core_news_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "python -m spacy download ja_core_news_sm\n",
    "python -m spacy download lt_core_news_sm\n",
    "python -m spacy download ru_core_news_sm\n",
    "python -m spacy download es_core_news_sm\n",
    "python -m spacy download uk_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup [wandb](https://docs.wandb.ai/guides/integrations/huggingface)** (optional)\n",
    "\n",
    "\n",
    "1. sign up for an account in [wandb](https://wandb.ai/)\n",
    "2. create a `wandb` folder in the root of this repository\n",
    "3. set TMPDIR to the path of `wandb` folder\n",
    "3. login in local setting `wandb login` and follow the instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pre-training:\n",
    "\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/lcquad2/train.json \\\n",
    "-o datasets/lcquad2/train.csv \\\n",
    "-t lcquad2 \\\n",
    "--linguistic_context \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 32 \\\n",
    "--entity_padding_length 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning:\n",
    "\n",
    "```bash\n",
    "python3 code/generate_train_csv.py \\\n",
    "-i datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.json \\\n",
    "-o datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.csv \\\n",
    "-t qald \\\n",
    "-kg Wikidata \\\n",
    "-l all \\\n",
    "--linguistic_context \\\n",
    "--entity_knowledge \\\n",
    "--question_padding_length 32 \\\n",
    "--entity_padding_length 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pre-train on LC-QuAD 2.0**\n",
    "\n",
    "Since pre-training and fine-tuning commands are based on the same script `train_ds.sh`, in order to save time and avoid errors, we directly provide the code in the configured bash script. You can run the following command in your terminal.\n",
    "\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero3.json \\\n",
    "    --model_name_or_path google/mt5-xl \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/lcquad2/train.csv \\\n",
    "    --output_dir fine-tuned_models/lcquad2-pretrain \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 6000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name lcquad2-pretrain \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fine-tune on QALD-9-Plus**\n",
    "\n",
    "```bash\n",
    "deepspeed --include=localhost:0 --master_port 60000 code/train_new.py \\\n",
    "    --deepspeed deepspeed/ds_config_zero3.json \\\n",
    "    --model_name_or_path fine-tuned_models/lcquad2-pretrain \\\n",
    "    --do_train \\\n",
    "    --train_file datasets/qald9plus/wikidata/qald_9_plus_train_wikidata.csv \\\n",
    "    --output_dir fine-tuned_models/qald9plus-finetune \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 3000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --report_to wandb \\\n",
    "    --run_name qald9plus-finetune \\\n",
    "    --tf32 1 \\\n",
    "    --fp16 0 \\\n",
    "    --gradient_checkpointing 1 \\\n",
    "    --gradient_accumulation_steps 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GERBIL Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `eval.sh` to generate prediction files in QALD format and evaluate them with GERBIL.\n",
    "`eval.sh` is configured. \n",
    "\n",
    "```bash\n",
    "./eval.sh\n",
    "```\n",
    "\n",
    "Prediction files are stored in `pred_files/exp9-fine-tune`.\n",
    "The script uploads them to GERBIL along with the reference test file\n",
    "and waits for 5 minutes for the results.\n",
    "If the GERBIL experiment terminates, the results are stored in `pred_files/exp9-fine-tune/result.csv`, else, the experiment id is stored in this file. You can use the following commands to generate a csv files for results:\n",
    "\n",
    "```bash\n",
    "python3 code/gerbil_eval.py --experiment_id [experiment_id] --pred_path pred_files/exp9-fine-tune\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
